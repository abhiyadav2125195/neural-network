{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65433603-cb04-4af3-bad5-19d7048cb916",
   "metadata": {},
   "source": [
    "# Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd0659-30d8-4525-b278-4d5614aaa2bb",
   "metadata": {},
   "source": [
    "In artificial neural networks, an activation function is a mathematical operation applied to the output of a neuron to determine its final output. It introduces non-linearities into the network, allowing it to learn complex patterns and relationships in the data. Activation functions help the neural network to make decisions and predictions based on the input it receives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9692fb-d14e-402f-8760-2913350bcce8",
   "metadata": {},
   "source": [
    "# Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974819f-289a-4ce7-9c1e-b1cea5db1bdd",
   "metadata": {},
   "source": [
    "Some common activation functions include:\n",
    "\n",
    "Sigmoid: Squeezes the output between 0 and 1.\n",
    "Hyperbolic Tangent (tanh): Similar to the sigmoid but ranges between -1 and 1.\n",
    "Rectified Linear Unit (ReLU): Outputs the input for positive values and zero for negative values.\n",
    "Leaky ReLU: Similar to ReLU but allows a small, non-zero gradient for negative inputs.\n",
    "Softmax: Used in the output layer for multi-class classification, converting raw scores into probabilities.\n",
    "Exponential Linear Unit (ELU): A variant of ReLU that allows negative values with a smooth transition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5bee29-c2bd-447a-bddc-bbf0022d7c92",
   "metadata": {},
   "source": [
    "# Q3. How do activation functions affect the training process and performance of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb77f49-edc8-4952-8648-8b7be0e90283",
   "metadata": {},
   "source": [
    "Activation functions introduce non-linearities to the network, enabling it to learn complex relationships in the data. The choice of activation function can impact the network's ability to converge during training, handle vanishing or exploding gradients, and model complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b990045-95c9-490b-8805-38771726bb14",
   "metadata": {},
   "source": [
    "# Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe082c6-33c7-4605-bb15-9246263dd177",
   "metadata": {},
   "source": [
    "The sigmoid activation function squashes input values to a range between 0 and 1. It's useful in the output layer for binary classification problems. However, it suffers from the vanishing gradient problem, where the gradients become very small for extreme input values, making it challenging for the network to learn.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Outputs are in a interpretable range (0 to 1).\n",
    "Useful in the output layer for binary classification.\n",
    "Disadvantages:\n",
    "\n",
    "Prone to vanishing gradient problem.\n",
    "Outputs are not centered around zero, which can slow down learning in subsequent layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a182886-b77a-454d-8284-40d013974b8c",
   "metadata": {},
   "source": [
    "# Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e089a-5bcc-478e-a92e-f9cc86816596",
   "metadata": {},
   "source": [
    "ReLU sets the output to zero for negative input values and leaves positive values unchanged. Unlike the sigmoid, it is not bound to a specific output range and allows the model to learn faster. It helps mitigate the vanishing gradient problem and is computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a36506-544a-44ea-a7a9-5ca117bcfc9e",
   "metadata": {},
   "source": [
    "# Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0263ba-f45f-45ed-bc57-b3465d37e74c",
   "metadata": {},
   "source": [
    "Benefits of ReLU over sigmoid include:\n",
    "\n",
    "Mitigates vanishing gradient problem.\n",
    "\n",
    "Computationally efficient due to simple mathematical operations.\n",
    "\n",
    "Allows the network to converge faster during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1eb0a-c359-4e8d-b577-410ab63841cf",
   "metadata": {},
   "source": [
    "# Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d32ac2-4fe0-4d47-8f41-a2a4f2a8747a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8495643a-97e4-4883-9a31-29edd335c1ed",
   "metadata": {},
   "source": [
    "# Q8. What is the purpose of the softmax activation function? When is it commonly used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d25a8-f5ca-4b8d-90c6-eacd86e4da8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3843c0cc-ce53-4560-bd6b-f5227186d040",
   "metadata": {},
   "source": [
    "# Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3228b-d6cf-440b-9763-6c2c2e17c23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
